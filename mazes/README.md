# Dyna-Q and Prioritized Sweeping Algorithms

This project simulates and compares key reinforcement learning algorithms, Dyna-Q and Prioritized Sweeping, in a maze environment. It specifically focuses on their performance in non-stationary environments, where the optimal path can change over time

---

## Objective

To understand how reinforcement learning agents, using different planning mechanisms, adapt to a changing environment. The project compares the learning efficiency of a standard Dyna-Q agent against an enhanced Dyna-Q+ agent and a Prioritized Sweeping agent.

---

## Algorithms & Concepts

**A brief explanation of the key theoretical concepts implemented.**

- Dyna-Q: Combines model-free learning (from direct experience) with model-based planning (from simulated experience). It learns faster by using a learned model of the environment to practice and update its policy.

- Dyna-Q+: An extension of Dyna-Q designed to handle non-stationary environments. It introduces a bonus reward for exploring state-action pairs that have not been visited for a long time, thus encouraging the agent to discover environmental changes.

- Prioritized Sweeping: A more advanced planning algorithm that prioritizes updates based on the magnitude of change in state values. This method focuses computational resources on the most important updates, leading to faster and more efficient learning.

---

## Usage

- `dyna_maze.ipynb:` This notebook demonstrates the power of Dyna-Q in a simple, static maze environment. It shows that agents with more planning steps (n=5,50) learn much faster than a non-planning agent (n=0) by using simulated experience to improve their policy.

- `changing_maze.ipynb:` This notebook demonstrates how Dyna-Q and Dyna-Q+ agents handle a non-stationary environment. It compares their performance in two scenarios:

-  Blocking Maze: The initial optimal path is blocked, forcing the agents to find a new route.

- Shortcut Maze: A new, shorter path opens up, and the notebook highlights how only the Dyna-Q+ agent, with its curiosity-driven bonus, is able to discover and switch to this new path.

- `prioritized_sweeping.ipynb:` (Incomplete) This notebook is intended to implement and demonstrate the Prioritized Sweeping algorithm.

## Results

- The plots generated by the notebooks show the cumulative reward of the agents over time.

- In the Shortcut Maze, the Dyna-Q+ agent's cumulative reward graph shows a clear upward shift, indicating it successfully found the new, shorter path. The standard Dyna-Q agent, however, remains on the original path.

- The results demonstrate that while Dyna-Q is efficient, Dyna-Q+ is a more robust solution for real-world scenarios where the environment is not static.

