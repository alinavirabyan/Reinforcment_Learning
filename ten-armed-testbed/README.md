ğŸ° **What is the 10-Armed Bandit Problem?**  
- ğŸ¯ A basic problem in reinforcement learning.  
- ğŸ•¹ï¸ You choose from **10 actions (arms)** repeatedly.  
- ğŸ’° Each action gives a **random reward** based on an unknown distribution.  
- ğŸ§  The goal is to **maximize total reward** over time.

---

ğŸ” **Key Concepts**  
- ğŸ“Š **Evaluative Feedback**: You learn only from rewards, not from being told the right action.  
- âš–ï¸ **Exploration vs Exploitation**:  
  - ğŸš€ *Exploration*: Try new actions to discover their value.  
  - ğŸ’ *Exploitation*: Use the best-known action to get rewards.

---

ğŸ§® **How to Estimate Action Values**  
- â• Take the **average** of the rewards received from each action.  
- ğŸŸ¢ **Greedy method**: Always pick the action with the highest current value.  
- ğŸ” **Îµ-Greedy method**:  
  - ğŸ’¡ Choose a random action **Îµ%** of the time.  
  - âœ… Choose the best-known action **(1 - Îµ)%** of the time.

---

ğŸ§ª **10-Armed Testbed Experiment**  
- ğŸ”¢ Run on **2000 random problems**.  
- ğŸ“ˆ Îµ-Greedy (with Îµ = 0.1 or 0.01) performed **better than greedy**, because:  
  - ğŸŒ± They explored more.  
  - ğŸ§  Found better actions over time.

